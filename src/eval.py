"""Evaluation script for the social bias model."""

import argparse
import logging
import os
import re

import numpy as np
import torch
from data.get_dataset import get_social_bias_dataset
from datasets import Dataset
from sklearn.metrics import accuracy_score
from tabulate import tabulate
from utils import PREDS_CACHE_DIR, PROMPTS_DIR, generate_predictions


def classify(prediction: str, labels: set[str]) -> str:
    """
    Extract the label from the response.

    Args:
        response (str): The response from the model.
        labels (set[str]): The set of possible labels.

    Returns:
        str: A label in the given set of labels, or an alternative string if
            a fitting label could not be found. These alternative strings can
            be useful for getting insight into the model's outputs.
    """
    prediction = prediction.lower()

    # if re.search(r"\bfoo\b", prediction) is not None:
    #     return "yes"
    # if re.search(r"\bbar\b", prediction) is not None:
    #     return "no"

    labels_in_prediction = set()
    for label in labels:
        if re.search(rf"\b{label.lower()}\b", prediction.lower()) is not None:
            labels_in_prediction.add(label)

    # Single label in prediction.
    if len(labels_in_prediction) == 1:
        return labels_in_prediction.pop()

    # Multiple labels in prediction.
    if len(labels_in_prediction) > 1:
        return "/".join(sorted(labels_in_prediction))

    # No labels in prediction (`len(labels_in_prediction) == 0`).
    return "None"


def extract_labels(
    dataset: Dataset, predictions: list[str]
) -> tuple[list[str], list[str]]:
    """
    Extract the predicted and ground truth labels from the dataset.

    Args:
        dataset (Dataset): The dataset to extract the labels from.
        predictions (list[str]): The predictions generated by the model.

    Returns:
        Tuple containing:
            list[str]: The ground truth labels.
            list[str]: The predicted labels.
    """
    logging.info("Extracting predicted and ground truth labels...")
    labels_true = dataset["offensiveYN"][: len(predictions)]
    labels = set(labels_true)
    labels_pred = [classify(prediction, labels) for prediction in predictions]
    return labels_true, labels_pred


def confusion_matrix(
    labels_true: list[int], labels_pred: list[int]
) -> tuple[np.ndarray, list[str]]:
    """
    Compute the confusion matrix.

    Args:
        labels_true (list[int]): The true labels.
        labels_pred (list[int]): The predicted labels.

    Returns:
        Tuple containing:
            np.ndarray: The confusion matrix, where the rows represent the true
                labels and the columns represent the predicted labels.
            list[str]: The labels for the rows.
            list[str]: The labels for the columns.
    """
    row_labels = sorted(set(labels_true))
    col_labels = sorted(set(labels_pred))

    # Map the row/column indices to labels.
    rowlabel2idx = {label: idx for idx, label in enumerate(row_labels)}
    collabel2idx = {label: idx for idx, label in enumerate(col_labels)}

    # Initialize the confusion matrix.
    cm = np.zeros((len(row_labels), len(col_labels)), dtype=int)

    # Fill the confusion matrix.
    for label_true, label_pred in zip(labels_true, labels_pred):
        cm[rowlabel2idx[label_true], collabel2idx[label_pred]] += 1

    return cm, row_labels, col_labels


def evaluate(dataset: Dataset, predictions: list[str]) -> dict:
    """
    Evaluate the generated predictions on the dataset.

    Args:
        dataset (Dataset): The dataset to evaluate the model on.
        predictions (list[str]): The predictions generated by the model.

    Returns:
        Dict containing:
            confusion_matrix (tuple[np.ndarray, list[str], list[str]]): The
                confusion matrix, where the rows represent the true labels and
                the columns represent the predicted labels.
            accuracy (float): The accuracy of the model.
    """
    # Extract predicted and ground truth labels.
    labels_true, labels_pred = extract_labels(dataset, predictions)

    # Finally evaluate the predictions.
    logging.info(f"Evaluating model on {len(labels_true)} examples...")
    return {
        "confusion_matrix": confusion_matrix(labels_true, labels_pred),
        "accuracy": accuracy_score(labels_true, labels_pred),
    }


def show_metrics(metrics: dict) -> None:
    """
    Show the metrics produced by the evaluation function.

    Args:
        metrics (dict): The metrics produced by the evaluation function.
    """
    # Show the confusion matrix.
    cm, row_labels, col_labels = metrics["confusion_matrix"]
    cm = np.vstack((col_labels, cm))
    row_labels.insert(0, r"True " + "\u2193" + " Predicted " + "\u2192")
    cm_table = tabulate(cm, showindex=row_labels, tablefmt="fancy_grid")
    logging.info(f"Confusion matrix:\n{cm_table}")

    # Show the quantitative metrics.
    logging.info(f"Accuracy: {metrics['accuracy']:0.3f}")


def main(args: argparse.Namespace) -> None:
    """
    Main function for the evaluation script.

    Args:
        args (argparse.Namespace): The command line arguments.
    """
    logging.info(f"Args: {args}")

    # Set the random seed.
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # Extract the prompt.
    with open(args.prompt_path, "r") as f:
        prompt = f.read()

    # Load the dataset.
    dataset = get_social_bias_dataset(args.split, num_workers=args.num_workers)

    # Generate the predictions.
    predictions = generate_predictions(
        args.model,
        args.split,
        prompt,
        dataset,
        min_preds=args.min_preds,
        preds_cache_dir=args.preds_cache_dir,
        save_every=args.save_every,
    )

    # Evaluate the model.
    metrics = evaluate(dataset, predictions)

    # Show the metrics.
    show_metrics(metrics)


if __name__ == "__main__":
    # Set up logging.
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
        datefmt="%H:%M:%S",
    )

    # Create the argument parser.
    parser = argparse.ArgumentParser()

    # Required parameters.
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        # For up-to-date model names, see https://huggingface.co/MBZUAI.
        choices=[
            "LaMini-Cerebras-111M",
            "LaMini-Cerebras-256M",
            "LaMini-Cerebras-590M",
            "LaMini-Cerebras-1.3B",
            "LaMini-GPT-774M",
            "LaMini-GPT-124M",
            "LaMini-GPT-1.5B",
            "LaMini-Neo-125M",
            "LaMini-Neo-1.3B",
            "LaMini-Flan-T5-783M",
            "LaMini-Flan-T5-248M",
            "LaMini-Flan-T5-77M",
            "LaMini-T5-738M",
            "LaMini-T5-223M",
            "LaMini-T5-61M",
            "bactrian-x-7b-lora",
            "swiftformer-l1",
            "swiftformer-l3",
            "swiftformer-s",
            "swiftformer-xs",
        ],
        help="The name of the model to evaluate.",
    )

    # Optional parameters.
    parser.add_argument(
        "--seed", type=int, default=42, help="Random seed. Defaults to 42."
    )
    parser.add_argument(
        "--split",
        type=str,
        choices=["train", "val", "test"],
        default="test",
        help="Split to evaluate on. Defaults to 'test'.",
    )
    parser.add_argument(
        "--prompt_path",
        type=str,
        default=os.path.join(PROMPTS_DIR, "default.txt"),
        help="Path to the prompt to use for evaluation. Defaults to "
        f"'{os.path.join(PROMPTS_DIR, 'default.txt')}'.",
    )
    parser.add_argument(
        "--min_preds",
        type=int,
        default=0,
        help="The minimum amount of predictions that should be generated. If "
        "the cache file already contains some of these predictions, the model "
        "will continue generating predictions until the minimum amount is "
        "reached. If 0, all predictions will be generated. Defaults to 0.",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=4,
        help="Number of workers to use for data loading. If set to 0, "
        "no multiprocessing will be used. Defaults to 4.",
    )
    parser.add_argument(
        "--preds_cache_dir",
        type=str,
        default=PREDS_CACHE_DIR,
        help="Path to the directory where the predictions cache should be "
        f"stored. Defaults to '{PREDS_CACHE_DIR}'.",
    )
    parser.add_argument(
        "--save_every",
        type=int,
        default=1000,
        help="Cache the predictions every n predictions. Defaults to 1000.",
    )

    # Parse the arguments.
    args = parser.parse_args()
    main(args)
