"""Evaluates a GPT model on the Social Bias dataset."""

import argparse
import os
import threading

from datasets import Dataset
from gpt.generate_gpt import get_chat_completion
from tqdm import tqdm
from tqdm.contrib.concurrent import thread_map
from utils import (
    DIR_CACHES,
    DIR_IMAGES,
    DIR_PROMPTS,
    extract_prompt,
)

from data.get_dataset import get_social_bias_dataset


def append_predictions_to_file(
    predictions: list[dict],
    model: str,
    split: str,
    prompt_type: str,
    dir_caches: str,
) -> None:
    """Appends predictions to a file.

    Args:
        predictions: A list of predictions. Each prediction is a dictionary with
            the following keys:
                - prompt: The prompt used to generate the completion.
                - completion: The completion generated by the model.
        model: The name of the model.
        split: The name of the split.
        prompt_type: The name of the prompt.
        dir_caches: The path to the directory where the cache files are stored.
    """
    with open(f"{dir_caches}/{model}_{split}_{prompt_type}.jsonl", "a") as f:
        for prediction in predictions:
            f.write(f"{prediction}\n")


def generate_predictions(
    dataset: Dataset,
    prompt_template: str,
    model: str,
    split: str,
    save_every: int,
    dir_caches: str,
    prompt_type: str,
    lock: threading.Lock,
) -> None:
    """Generates predictions for a dataset."""
    predictions = []

    for example_idx, example in enumerate(
        tqdm(
            dataset,
            desc="Generating predictions",
            unit="preds",
        ),
        start=1
    ):
        prompt, message = prepare_prompt(prompt_template, example)
        if prompt_already_processed(prompt, lock, dir_caches, model, split, prompt_type):
            continue

        if "{explanation}" in prompt_template:
            explanation = get_chat_completion(
                message, model, temperature=0.7, max_tokens=1000
            ).strip()
            prompt, message = prepare_prompt(prompt_template, example, explanation)

        completion = get_chat_completion(
            message, model, temperature=0.7, max_tokens=1000
        ).strip()

        predictions.append({"prompt": prompt, "completion": completion})

        if example_idx % save_every == 0:
            with lock:
                append_predictions_to_file(predictions, model, split, prompt_type, dir_caches)
            predictions = []

    if predictions:
        with lock:
            append_predictions_to_file(predictions, model, split, prompt_type, dir_caches)


def prepare_prompt(
    prompt_template: str, example: dict, explanation: str = None
) -> tuple[str, list[dict]]:
    """Prepares the prompt for the model."""
    prompt = prompt_template
    if explanation is not None:
        prompt = prompt.replace("{explanation}", explanation)
    prompt = prompt.replace("{post}", example["post"]).strip()
    message = [{"role": "user", "content": prompt}]
    return prompt, message



def prompt_already_processed(
    prompt: str,
    lock: threading.Lock,
    dir_caches: str,
    model: str,
    split: str,
    prompt_type: str,
) -> bool:
    """Checks if the prompt has already been processed."""
    with lock:
        with open(
            f"{dir_caches}/{model}_{split}_{prompt_type}.jsonl", "r"
        ) as f:
            for line in f:
                if prompt in line:
                    return True
    return False

def main(args: argparse.Namespace) -> None:
    """Runs the evaluation pipeline."""
    lock = threading.Lock()

    # Ensure the caches directory exists
    os.makedirs(args.dir_caches, exist_ok=True)

    # Ensure the cache file exists
    cache_file_path = f"{args.dir_caches}/{args.model}_{args.split}_{args.prompt_name}.jsonl"
    if not os.path.isfile(cache_file_path):
        open(cache_file_path, "w").close()

    # Load the dataset.
    dataset_whole = get_social_bias_dataset(args.split, num_workers=args.num_workers)
    datasets = [dataset_whole.shard(args.chunks, i) for i in range(args.chunks)]
    print("Number of examples in each dataset: ")
    for dataset in datasets:
        print(len(dataset))

    # Load the prompt.
    prompt_template = extract_prompt(args.dir_prompts, args.prompt_name)

    # Generate predictions.
    thread_map(
        lambda dataset: generate_predictions(
            dataset,
            prompt_template,
            args.model,
            args.split,
            args.save_every,
            args.dir_caches,
            args.prompt_name,
            lock
        ),
        datasets,
    )

if __name__ == "__main__":
    """Parses the arguments and runs the evaluation pipeline."""
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Required arguments.
    # Common values (I put them here so we can easily copy-paste them):
    # gpt-3.5-turbo
    # gpt-4
    parser.add_argument(
        "--model",
        required=True,
        type=str,
        help="The name of the model to evaluate.",
    )
    # Common values (I put them here so we can easily copy-paste them):
    # default_0shot_no_cot
    # default_0shot_cot_expl_first
    # default_4shot_cot_ans_first
    # default_4shot_cot_expl_first
    parser.add_argument(
        "--prompt_name",
        required=True,
        type=str,
        help="Name of the prompt. The path to the prompt file is assumed to "
        "be f'{args.dir_prompts}/{args.prompt_name}.txt'.",
    )
    # Optional arguments.
    parser.add_argument(
        "--logging_level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="The logging level to use.",
    )
    parser.add_argument(
        "--chunks",
        type=int,
        default=5,
        help="Number of chunks to split the dataset into. Each chunk will be "
        "processed in a separate thread.",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=1,
        help="Number of workers to use for data loading. If set to 0, "
        "multiprocessing will not be used.",
    )
    parser.add_argument(
        "--split",
        type=str,
        choices=["train", "val", "test"],
        default="test",
        help="The split of the dataset to evaluate on.",
    )
    parser.add_argument(
        "--save_every",
        type=int,
        default=20,
        help="Number of predictions to generate before caching them.",
    )
    parser.add_argument(
        "--dir_caches",
        type=str,
        default=DIR_CACHES,
        help="Path to the directory where the caches are stored.",
    )
    parser.add_argument(
        "--dir_images",
        type=str,
        default=DIR_IMAGES,
        help="Path to the directory where the images are saved.",
    )
    parser.add_argument(
        "--dir_prompts",
        type=str,
        default=DIR_PROMPTS,
        help="Path to the directory where the prompts are saved.",
    )

    # Parse the arguments.
    args = parser.parse_args()


    main(args)



