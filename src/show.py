"""Script to print generated model predictions."""

import argparse
import logging
import os

import torch
from datasets import Dataset


# isort: off
from data.get_dataset import get_social_bias_dataset
from eval import evaluate, extract_labels, show_metrics
from utils import (
    PREDS_CACHE_DIR,
    PROMPTS_DIR,
    extract_cache_file_info,
    load_predictions,
)

# isort: on


def show_preds(
    prompt: str, dataset: Dataset, predictions: list[str], show_preds: int = 0
) -> None:
    """Show a subset of the predictions.

    Args:
        prompt (str): The prompt used to generate the predictions.
        dataset (Dataset): The dataset used to generate the predictions.
        predictions (list[str]): The predictions generated by the model.
        show_preds (int, optional): The number of generated predictions that
            should be shown. If 0, all predictions will be shown. Defaults to
            0.
    """
    if show_preds > len(predictions):
        logging.warning(
            f"The number of predictions to show ({show_preds}) is "
            "greater than the number of predictions generated for this model "
            f"({len(predictions)}). It will be decreased accordingly."
        )
    elif show_preds == 0:
        show_preds = len(predictions)

    labels_true, labels_pred = extract_labels(dataset, predictions)

    logging.info(f"Prompt:\n{prompt}")
    logging.info(f"Showing first {show_preds} generated by {args.model}:")
    for i, prediction in enumerate(predictions[:show_preds]):
        logging.info(f" Example {i} ".center(80, "-"))
        logging.info(f"Post: {dataset[i]['post']}")
        logging.info(f"Model's output: {prediction}")
        logging.info(f"True label: {labels_true[i]}")
        logging.info(f"Predicted label: {labels_pred[i]}")
        logging.info("")


def main(args: argparse.Namespace) -> None:
    """Main function for the evaluation script.

    Args:
        args (argparse.Namespace): The command line arguments.
    """
    logging.info(f"Args: {args}")

    # Set the random seed.
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # Extract the prompt.
    with open(args.prompt_path, "r") as f:
        prompt = f.read()

    # Load the dataset.
    dataset = get_social_bias_dataset(args.split, num_workers=args.num_workers)

    # Generate the predictions.
    predictions = load_predictions(
        args.model, args.split, prompt, preds_cache_dir=args.preds_cache_dir
    )

    # Show a subset of the predictions.
    show_preds(prompt, dataset, predictions, show_preds=args.show_preds)

    # Evaluate the model.
    metrics = evaluate(dataset, predictions)

    # Show the metrics.
    show_metrics(metrics)


if __name__ == "__main__":
    # Set up logging.
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
        datefmt="%H:%M:%S",
    )

    # Create the argument parser.
    parser = argparse.ArgumentParser()

    # Required parameters.
    parser.add_argument(
        "--preds_cache_file",
        type=str,
        help="The path to a predictions cache file. By default, the cache "
        "file location is determined automatically using other arguments. "
        "However, if this argument is provided, it will override the default "
        "cache file location. In this case, you should not provide the "
        "--model, --split, and --prompt_path arguments.",
    )
    parser.add_argument(
        "--model",
        type=str,
        # For up-to-date model names, see https://huggingface.co/MBZUAI.
        choices=[
            "LaMini-Cerebras-111M",
            "LaMini-Cerebras-256M",
            "LaMini-Cerebras-590M",
            "LaMini-Cerebras-1.3B",
            "LaMini-GPT-774M",
            "LaMini-GPT-124M",
            "LaMini-GPT-1.5B",
            "LaMini-Neo-125M",
            "LaMini-Neo-1.3B",
            "LaMini-Flan-T5-783M",
            "LaMini-Flan-T5-248M",
            "LaMini-Flan-T5-77M",
            "LaMini-T5-738M",
            "LaMini-T5-223M",
            "LaMini-T5-61M",
            "bactrian-x-7b-lora",
            "swiftformer-l1",
            "swiftformer-l3",
            "swiftformer-s",
            "swiftformer-xs",
        ],
        help="The name of the model that was evaluated.",
    )

    # Optional parameters.
    parser.add_argument(
        "--seed", type=int, default=42, help="Random seed. Defaults to 42."
    )
    parser.add_argument(
        "--split",
        type=str,
        choices=["train", "val", "test"],
        default="test",
        help="Split on which the model was evaluated. Defaults to 'test'.",
    )
    parser.add_argument(
        "--prompt_path",
        type=str,
        default=os.path.join(PROMPTS_DIR, "default.txt"),
        help="Path to the prompt that was used for evaluation. Defaults to "
        f"'{os.path.join(PROMPTS_DIR, 'default.txt')}'.",
    )
    parser.add_argument(
        "--show_preds",
        type=int,
        default=0,
        help="The number of generated predictions that should be shown. If 0, "
        "all predictions will be shown. Defaults to 0.",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=4,
        help="Number of workers to use for data loading. If set to 0, "
        "no multiprocessing will be used. Defaults to 4.",
    )
    parser.add_argument(
        "--preds_cache_dir",
        type=str,
        default=PREDS_CACHE_DIR,
        help="Path to the directory where the predictions cache should be "
        f"stored. Defaults to '{PREDS_CACHE_DIR}'.",
    )

    # Parse the arguments.
    args = parser.parse_args()

    # Check that exactly least one of the arguments --preds_cache_file or
    # --model is provided.
    if args.preds_cache_file is not None:
        if (
            args.model is not None
            or args.split != parser.get_default("split")
            or args.prompt_path != parser.get_default("prompt_path")
        ):
            raise ValueError(
                "If a value for --preds_cache_file, is provided, you should "
                "not provide values for --model, --split, and --prompt_path. "
                "Instead, these arguments will be automatically determined "
                "from the given cache file."
            )
        else:
            (
                args.model,
                args.split,
                args.prompt_path,
            ) = extract_cache_file_info(args.preds_cache_file)
    elif args.model is None:
        raise ValueError(
            "You must provide a value for either --preds_cache_file or "
            "--model."
        )

    main(args)
