# Integrating Chain-of-Thought Reasoning and Explainable AI for Enhanced Hate Speech Detection and Explanation Generation in Large Language Models

This repository investigates the integration of Chain-of-Thought (CoT) reasoning and Explainable AI (XAI) techniques for enhancing hate speech detection and explanation generation in large language models (LLMs). We aim to prompt LLMs to reason about potentially hateful content and produce a prediction or explanation, comparing inductive and deductive approaches for improved performance on hate speech datasets with full-text annotations. The primary dataset utilized for this project is SocialBiasFrames (Sap et al. 2019). We aim to provide human content moderators with more comprehensive and understandable explanations for detected hate speech, ultimately improving the moderation process and increasing the transparency of AI-driven decisions.

## Requirements

The code is written in Python 3.10. The requirements can be installed using `pip install -r requirements.txt` or with the conda environment file `conda env create -f environment.yml`.